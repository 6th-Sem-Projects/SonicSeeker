Hey everyone, here's Aski Otis from NMMIT Karnataka and today we are super excited to present our project for Hackfish 2025 under the open innovation track. Our struggle with hours of record in meetings, lectures or interviews, manually listening, taking notes and searching for key points is very exhausting right? But what if there is a tool that could do the hard work for you? Well that's exactly what we built. Think about it, every day we attend meetings, listen to podcasts and watch lectures. But when we need to extract key insights, it's a nightmare. Let's be real, listening to an entire recording just to find one important moment, it's a huge waste of time right? Manually summarizing long discussions, super inefficient, skipping through videos, randomly hoping to find key parts, it's very frustrating. So we ask ourselves, what if an AI could take care of this? And this is exactly how our AI Power Media Processing tool was born. Our tool makes AI and video files searchable, interactive and insightful. Here so, Whisper AI, think of this as a super smart subtitles for your media files. Isn't the speech and turns it into text with high accuracy, even with background noise or multiple accents. Weirder sentiment analysis, it detects emotions in speech. Was the speaker happy, neutral, upset? This is great for analyzing customer calls, debates or interviews. Spaceinner automatically detects names, places and key topics. Let's say if someone mentions NASA or AI or let's say Elon Musk, it highlights them instantly, like it's useful for research and organization purposes. But that's not all. They pack this tool with interactive features. We can search any keyword to jump into an important moment. Just type in a word like budget in a meeting and that's it. It takes you there instantly. We also have clickable timestamps. You must have seen this feature in YouTube, where every sentence in a transcript has a timestamp. Click on it and an audio or reader jumps it at exact moment. Also, we do not have to go through the entire file. The AI picks out the key points and gives you a quick summary. Whether it's a podcast, online class or business meeting, the tool works perfectly with both. If multiple people are talking, the AI identifies different voices and labels who said what, like speaker 1 and speaker 2. You also have a special unique feature called the mind map, where instead of reading a wall of text, we basically get a virtual roadmap of the entire conversation, making it easier to understand key topics and connectors. Now, let's discuss the tech stack. We'll be using XJS and TypeScript for a smooth user interface, with Node.json express for handling the backend. We're also trying to run this as locally as possible. Like we're using OpenIS whisper local model for transcription, video for sentiment analysis and space inert, just like I said before. And as for database for storing the bookmarks and timestamps, we'll be using PostgreSQL and hopefully scale it to NeonDB. Video processing, we'll be using FFMEG, VideoGS and WaveCipherGS for seamless payback. This tool is designed for anyone who deals with audio or video content. In a corporation, this can be used to extract key decisions from long meetings. Students could use this to summarize lectures and study smart. A legal team could generate automatic transcripts for key studies. Content creators could repurpose the best moments. A support team could analyze customer interactions for a better service. We believe that AI can transform how we process audio and video content, making it more accessible, interactive and intelligent. We're hoping that this could be a great tool that could help a lot of people. And that's a wrap-up for a short video of expanding our project. Hopefully, we'll see you in Hackfish. Goodbye for now.